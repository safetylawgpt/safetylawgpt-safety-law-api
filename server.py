# server.py â€” v5.8 ì‘ë‹µ ê°•ì œ / ì„¸ê·¸ë¨¼íŠ¸(í•­Â·í˜¸Â·ëª©) ë¶„ë¦¬ / ì „ìˆ˜ìŠ¤ìº”(ì„¸ê·¸ë¨¼íŠ¸ ìš°ì„ ) / ë§í¬ ë¹Œë”
import os, glob, yaml, re, datetime, urllib.parse
from typing import List, Dict, Tuple, Optional
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build

# ---------- í™˜ê²½ë³€ìˆ˜ ----------
LAWS_DIR      = os.getenv("LAWS_DIR", "./laws")
SHEETS_ID     = os.getenv("SHEETS_SPREADSHEET_ID")
SHEETS_RANGE  = os.getenv("SHEETS_RANGE", "ALL")  # ALL ë˜ëŠ” 'íƒ­ëª…!A:Z, ë‹¤ë¥¸íƒ­!A:Z'
GOOGLE_CREDS  = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

DISCLAIMER = (
    "ë³¸ ë‹µë³€ì€ ì•ˆì „ë²•ë„ìš°ë¯¸ GPTê°€ ìƒì„±í•œ ì¼ë°˜ ì •ë³´ì´ë©°, ë²•ë¥  ìë¬¸ì´ë‚˜ ë²•ë¥ ì‚¬ë¬´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
    "íŠ¹ì • ì‚¬ì‹¤ê´€ê³„ì— ëŒ€í•œ í•´ì„Â·ì ìš©ì€ ê´€í• ê¸°ê´€ì˜ ê³µì‹ ì•ˆë‚´ì™€ ìê²© ìˆëŠ” ë³€í˜¸ì‚¬Â·ë…¸ë¬´ì‚¬ì˜ ìë¬¸ìœ¼ë¡œ ê²€ì¦í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. "
    "ë³¸ ëŒ€í™”ëŠ” ë³€í˜¸ì‚¬â€“ì˜ë¢°ì¸ ê´€ê³„ë¥¼ í˜•ì„±í•˜ì§€ ì•Šìœ¼ë©°, ì •ë³´ì˜ ìµœì‹ ì„±Â·ì™„ì „ì„±Â·ì í•©ì„±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
    "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê²°ì •ê³¼ ì‹¤í–‰ì˜ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤."
)

app = FastAPI(title="SafetyLawGPT API", version="1.2.0")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

LAWS: List[Dict] = []  # ë©”ëª¨ë¦¬ DB (ì‹œíŠ¸ ìš°ì„ , YAML ë³´ì¡°)

# ---------- ìœ í‹¸ ----------
def _strip_html(s: Optional[str]) -> str:
    return re.sub("<[^>]+>", "", s or "")

def _now_iso() -> str:
    return datetime.datetime.now().astimezone().isoformat()

def _safe(s: Optional[str]) -> str:
    return (s or "").replace("\u00A0"," ").strip()

def _ellipsis(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else s[:n].rstrip() + "â€¦"

def _law_level(law_name: str) -> str:
    n = law_name or ""
    if "ì‹œí–‰ê·œì¹™" in n or "ê¸°ì¤€ì— ê´€í•œ ê·œì¹™" in n: return "rule"
    if "ì‹œí–‰ë ¹"  in n: return "decree"
    if "ê³ ì‹œ" in n or "ì§€ì¹¨" in n: return "notice"
    return "act"

def _safe_link(text: str, url: str) -> str:
    return f"[{text}](<{url}>)" if url else text

# ê°•í™”í˜• ê²€ìƒ‰ URL(ë²•ë ¹ íƒ­ ê³ ì •)
def _law_search_url(query: str) -> str:
    base = "https://www.law.go.kr/lsSc.do?section=&menuId=1&subMenuId=15&tabMenuId=81&eventGubun=060101&query="
    return base + urllib.parse.quote(_safe(query))

# ë§í¬ ë¹Œë”: lsId ì§í–‰ > (ì˜µì…˜)ì‹œíŠ¸ URL > ê²€ìƒ‰(ë²•ë ¹ëª…+ì¡°ë¬¸)
def _build_source_url(rec: Dict) -> str:
    lsid = _safe(rec.get("lsId") or rec.get("lsid") or rec.get("LSID"))
    if lsid:
        return f"https://www.law.go.kr/lsInfoP.do?lsId={urllib.parse.quote(lsid)}"
    src = _safe(rec.get("source_url"))
    if os.getenv("PREFER_SHEET_URL") == "1" and src.startswith("http") and "law.go.kr" in src:
        return src
    law_name = _safe(rec.get("law_name","")); article = _safe(rec.get("article_no",""))
    q = f"{law_name} {article}".strip()
    return _law_search_url(q if q else law_name)

ROLE_TOKENS = ["ì•ˆì „ê´€ë¦¬ì","ë³´ê±´ê´€ë¦¬ì","ì•ˆì „ë³´ê±´ì´ê´„ì±…ì„ì","ê´€ë¦¬ê°ë…ì","ì•ˆì „ë³´ê±´ê´€ë¦¬ë‹´ë‹¹ì","ì‚°ì—…ë³´ê±´ì˜"]
def _detect_role(q: str) -> Optional[str]:
    for r in ROLE_TOKENS:
        if r in q: return r
    return None

# ë¹ˆë„/ì£¼ê¸° ì „ìˆ˜ ìŠ¤ìº”(í‚¤ì›Œë“œ)
FREQ_PAT = re.compile(r"(ë°˜ê¸°\s*1íšŒ(?:\s*ì´ìƒ)?|ë°˜ê¸°|6\s*ê°œì›”(?:\s*1íšŒ(?:\s*ì´ìƒ)?)?|ë¶„ê¸°|ì •ê¸°)")
VERB_PAT = re.compile(r"(ì ê²€|í‰ê°€|ê´€ë¦¬|í™•ì¸|ê²€í† )")

# ---------- YAML ë³´ì¡° ë¡œë” ----------
def _load_yaml(path: str) -> Optional[Dict]:
    try:
        with open(path, "r", encoding="utf-8") as f: return yaml.safe_load(f)
    except Exception: return None

def load_from_yaml() -> List[Dict]:
    out=[]
    for y in glob.glob(os.path.join(LAWS_DIR, "**", "*.yml"), recursive=True):
        r=_load_yaml(y)
        if not r or not r.get("law_id") or not r.get("article_no"): continue
        r["_text"]=(r.get("text_plain") or _strip_html(r.get("text_html"))).strip()
        r["_source"]="yaml"; r["_level"]=_law_level(r.get("law_name",""))
        # YAMLì—ëŠ” ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ _segments ì—†ìŒ
        out.append(r)
    return out

# ---------- Sheets ë¡œë” (ALL íƒ­ + í•œêµ­ì–´ í—¤ë”) ----------
def _sheets_service():
    creds = Credentials.from_service_account_file(
        GOOGLE_CREDS, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
    )
    return build("sheets", "v4", credentials=creds)

def _ranges_from_env(svc):
    rng=(SHEETS_RANGE or "").strip()
    if rng.upper()=="ALL":
        meta=svc.spreadsheets().get(spreadsheetId=SHEETS_ID).execute()
        titles=[s["properties"]["title"] for s in meta.get("sheets",[])]
        return [f"{t}!A:Z" for t in titles]
    parts=[r.strip() for r in rng.split(",") if r.strip()]
    return parts if parts else ["Sheet1!A:Z"]

def _process_values(values: List[List[str]]) -> List[Dict]:
    if not values: return []
    header=[h.strip() for h in values[0]]; idx={k:i for i,k in enumerate(header)}
    def ci(*names):
        for n in names:
            if n in idx: return idx[n]
        return None
    # ì‹œíŠ¸ í—¤ë”
    c_rev=ci("ìµœì‹ ê°œì •ì¼","ê°œì •ì¼","revision_date")
    c_law_id=ci("ë²•ë ¹ID","law_id","ID")
    c_law_name=ci("ë²•ë ¹ëª…","law_name")
    c_article=ci("ì¡°ë¬¸ë²ˆí˜¸","article_no")
    c_unit=ci("ì¡°","unit","ì¡°ë¬¸êµ¬ë¶„","êµ¬ë¶„")
    c_title=ci("ì¡°ë¬¸ì œëª©","article_title")
    c_text=ci("ì¡°ë¬¸ë‚´ìš©(Plain)","text","ë³¸ë¬¸")
    c_html=ci("ì¡°ë¬¸ë‚´ìš©(HTML)","text_html")
    c_deleted=ci("ì‚­ì œì—¬ë¶€(Y/N)","ì‚­ì œì—¬ë¶€","ì‚­ì œ")
    c_src=ci("ì¶œì²˜URL","source_url","URL","url")
    c_lsid=ci("lsId","LSID","lsid","ë²•ì œì²˜ID")
    # ì„¸ê·¸ë¨¼íŠ¸ìš©
    c_para=ci("í•­ë²ˆí˜¸"); c_ho=ci("í˜¸ë²ˆí˜¸"); c_mok=ci("ëª©ë²ˆí˜¸")

    def g(row, i): 
        return (row[i].strip() if i is not None and i < len(row) and row[i] is not None else "")

    by_key: Dict[Tuple[str,str], Dict]={}

    for row in values[1:]:
        if _safe(g(row,c_deleted)).upper()=="Y": continue
        law_id=_safe(g(row,c_law_id)); law_name=_safe(g(row,c_law_name)); article_no=_safe(g(row,c_article))
        if not law_id or not article_no: continue

        unit=_safe(g(row,c_unit))
        title=_safe(g(row,c_title))
        rev=_safe(g(row,c_rev))
        src=_safe(g(row,c_src))
        lsid=_safe(g(row,c_lsid))
        t_plain=_safe(g(row,c_text))
        t_html=_safe(g(row,c_html))
        para=_safe(g(row,c_para)); ho=_safe(g(row,c_ho)); mok=_safe(g(row,c_mok))

        key=(law_id,article_no)
        is_head=("ì¡°" in unit) or (key not in by_key)

        if is_head:
            rec={
                "law_id":law_id,"law_name":law_name,"article_no":article_no,"article_title":title,
                "revision_date":rev,"db_synced_at":_now_iso().split("T")[0],"status":"ìœ íš¨",
                "source_url":src,"lsId":lsid,
                "text_plain":(t_plain+"\n") if t_plain else "", "text_html":(t_html+"\n") if t_html else "",
                "_source":"sheets","_level":_law_level(law_name),
                "_segments":[]  # í•­/í˜¸/ëª© ì„¸ê·¸ë¨¼íŠ¸ ëˆ„ì 
            }
            by_key[key]=rec
        else:
            # ë³¸ë¬¸ ëˆ„ì (ë°±ì—…ìš©)
            if t_plain: by_key[key]["text_plain"]+=t_plain+"\n"
            if t_html:  by_key[key]["text_html"] +=t_html+"\n"
            # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€
            seg_text = t_plain or _strip_html(t_html)
            if seg_text:
                by_key[key]["_segments"].append({
                    "para": para, "ho": ho, "mok": mok,
                    "text": seg_text
                })

    out=[]
    for rec in by_key.values():
        rec["_text"]=(rec.get("text_plain") or _strip_html(rec.get("text_html"))).strip()
        out.append(rec)
    return out

def load_from_sheets() -> List[Dict]:
    if not (SHEETS_ID and GOOGLE_CREDS): return []
    try:
        svc=_sheets_service(); ranges=_ranges_from_env(svc)
        resp=svc.spreadsheets().values().batchGet(spreadsheetId=SHEETS_ID, ranges=ranges).execute()
        valueRanges=resp.get("valueRanges",[])
        merged=[]
        for vr in valueRanges:
            merged.extend(_process_values(vr.get("values",[])))
        return merged
    except Exception:
        return []

# ---------- ì „ì²´ ë¦¬ë¡œë“œ ----------
def reload_all():
    global LAWS
    s=load_from_sheets(); y=load_from_yaml()
    seen=set(); merged=[]
    for rec in s+y:  # ì‹œíŠ¸ ìš°ì„ 
        key=(rec.get("law_id"), rec.get("article_no"))
        if key in seen: continue
        seen.add(key); merged.append(rec)
    LAWS=merged

reload_all()

# ---------- ê°„ë‹¨ ê²€ìƒ‰ ----------
def _score(hay: str, kw: str) -> int:
    score=0
    for t in kw.split():
        if t in hay: score+=hay.count(t)
    if kw in hay: score+=3
    return score

def _search_local(keyword: str, limit: int = 16) -> List[Dict]:
    kw=keyword.strip(); res=[]
    for r in LAWS:
        hay=f"{r.get('law_name','')} {r.get('article_no','')} {r.get('article_title','')} {r.get('_text','')}"
        sc=_score(hay, kw)
        if sc>0: res.append((sc,r))
    res.sort(key=lambda x:x[0], reverse=True)
    return [x[1] for x in res[:limit]]

# ---------- ì „ìˆ˜ ìŠ¤ìº”(ì„¸ê·¸ë¨¼íŠ¸ ìš°ì„ ) ----------
def _mk_path(article: str, para: str, ho: str, mok: str) -> str:
    path = article or ""
    if para: path += f"ì œ{para}í•­"
    if ho:   path += f"ì œ{ho}í˜¸"
    if mok:  path += f"{mok}ëª©"
    return path or article

def _scan_frequency_segments(rec: Dict) -> List[Tuple[str,str]]:
    """ì„¸ê·¸ë¨¼íŠ¸ì— ë¹ˆë„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ (ê²½ë¡œ, ìŠ¤ë‹ˆí«) ë°˜í™˜; ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸"""
    out: List[Tuple[str,str]] = []
    segs: List[Dict] = rec.get("_segments") or []
    for sg in segs:
        txt = (sg.get("text") or "").strip()
        if not txt: continue
        if FREQ_PAT.search(txt) and VERB_PAT.search(txt):
            snippet = re.sub(r"\s+"," ", txt)
            out.append((_mk_path(rec.get("article_no",""), sg.get("para",""), sg.get("ho",""), sg.get("mok","")), snippet))
    # ì¤‘ë³µ ì œê±°
    seen=set(); uniq=[]
    for p,s in out:
        k=(p, re.sub(r"\s+"," ",s))
        if k in seen: continue
        seen.add(k); uniq.append((p,s))
    return uniq

# ---------- ì—”ë“œí¬ì¸íŠ¸ ----------
@app.get("/healthz", operation_id="healthz")
def healthz():
    sheets=sum(1 for r in LAWS if r.get("_source")=="sheets")
    yaml_n=sum(1 for r in LAWS if r.get("_source")=="yaml")
    return {"ok": True, "ts": _now_iso(), "laws_loaded": len(LAWS), "sheets": sheets, "yaml": yaml_n}

@app.get("/reload", operation_id="reload")
def reload():
    reload_all()
    sheets=sum(1 for r in LAWS if r.get("_source")=="sheets")
    yaml_n=sum(1 for r in LAWS if r.get("_source")=="yaml")
    return {"ok": True, "reloaded": len(LAWS), "sheets": sheets, "yaml": yaml_n}

@app.get("/search", operation_id="search")
def search(keyword: str = Query(..., min_length=1), limit: int = 10):
    hits=_search_local(keyword, limit)
    return {"count": len(hits), "items":[
        {"law_id":h.get("law_id"),"law_name":h.get("law_name"),"article_no":h.get("article_no"),
         "title":h.get("article_title"),"revision_date":h.get("revision_date"),"db_synced_at":h.get("db_synced_at"),
         "level":h.get("_level"),"source":h.get("_source")}
        for h in hits]}

def _group_by_level(hits: List[Dict]) -> Dict[str, List[Dict]]:
    buckets={"act":[], "decree":[], "rule":[], "notice":[]}
    for r in hits: buckets.setdefault(r.get("_level","act"), []).append(r)
    for k in buckets: buckets[k]=buckets[k][:3]  # ë ˆë²¨ë³„ ìµœëŒ€ 3ê°œ
    return buckets

def _summarize(text: str) -> str:
    return _ellipsis(re.sub(r"\s+"," ", (text or "").strip()), 220)

def _basis_block_for(rec: Dict, scan_freq: bool=False) -> str:
    # [ê·¼ê±°] â€” ë°˜ë§ì²´
    law_name=rec.get("law_name",""); article=rec.get("article_no",""); title=rec.get("article_title","")
    rev=rec.get("revision_date",""); summary=_summarize(rec.get("_text",""))
    url=_build_source_url(rec)
    lines=[]
    lines.append(f"- **ë²•ë ¹ëª…:** {law_name}")
    lines.append(f"- **ì¡°ë¬¸:** {article}({title})")
    lines.append(f"- **ìµœì‹ ê°œì •ì¼:** {rev}")
    lines.append(f"- **ì›ë¬¸ ìš”ì§€:** {summary}")
    if scan_freq:
        # 1ìˆœìœ„: ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ìš©, 2ìˆœìœ„: ì—†ìŒ(ì„¸ê·¸ë¨¼íŠ¸ ì—†ìœ¼ë©´ ì¶œë ¥ ìƒëµ)
        matches=_scan_frequency_segments(rec)
        if len(matches)>=2:
            for path, snip in matches:
                mm=_ellipsis(snip, 140)
                lines.append(f"  - `{path}` â€” â€œ**{mm}**â€")
    lines.append(f"- **ì¶œì²˜:** {_safe_link('êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° ë°”ë¡œê°€ê¸°', url)}")
    return "\n".join(lines)

def _compose_blocks(keyword: str, role_lock: Optional[str], include_all_levels: bool, scan_frequency: bool) -> Tuple[str,str]:
    hits=_search_local(keyword, 16)
    if not hits:
        srch=_law_search_url(keyword)
        basis=f"ğŸ“Œ **[ê·¼ê±°]**\n- ì›ë¬¸ì„ ì°¾ì§€ ëª»í–ˆë‹¤. ë‚´ë¶€ DB(ì‹œíŠ¸/ë¡œì»¬)ì— í•´ë‹¹ ì¡°ë¬¸ì´ ì—†ë‹¤.\n- **ê²€ìƒ‰ ê²½ë¡œ:** {_safe_link('êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° ê²€ìƒ‰', srch)}"
        body=("**ë‚´ìš© ìš”ì•½**\n"
              "- ë²•ë¥  â†’ ì‹œí–‰ë ¹ â†’ ì‹œí–‰ê·œì¹™ â†’ ê³ ì‹œÂ·ì§€ì¹¨ ìˆœì„œë¡œ ìµœì‹  ì›ë¬¸ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.\n"
              "- ì¡°ë¬¸Â·ë³„í‘œ ì •í™• ì¼ì¹˜ í•­ëª©ë§Œ ì¸ìš©í•©ë‹ˆë‹¤.\n"
              "â€» ì¶”ê°€ í™•ì¸: ìƒÂ·í•˜ìœ„ë²• ê°œì •ì¼ì„ ê¼­ ë¹„êµí•´ ì£¼ì„¸ìš”.")
        return basis, body

    role = role_lock or _detect_role(keyword) or ""
    if role:
        role_hits=[r for r in hits if role in (r.get("_text","")+r.get("article_title","")+r.get("law_name",""))]
        if role_hits: hits=role_hits + [r for r in hits if r not in role_hits]

    buckets=_group_by_level(hits)
    order=["act","decree","rule","notice"] if include_all_levels else ["decree"]
    labels={"act":"(ë²•ë¥ )","decree":"(ì‹œí–‰ë ¹)","rule":"(ì‹œí–‰ê·œì¹™)","notice":"(ê³ ì‹œÂ·ì§€ì¹¨)"}

    basis_parts=["ğŸ“Œ **[ê·¼ê±°]**"]
    for lv in order:
        if not buckets.get(lv): continue
        for rec in buckets[lv]:
            basis_parts.append(f"- **{labels[lv]}**")
            basis_parts.append(_basis_block_for(rec, scan_freq=scan_frequency))
    basis_md="\n".join(basis_parts)

    body_lines=["**ë‚´ìš© ìš”ì•½**"]
    if role: body_lines.append(f"- ë³¸ ì§ˆì˜ëŠ” **{role}** ê´€ë ¨ìœ¼ë¡œ í•´ì„í–ˆìŠµë‹ˆë‹¤(ì—­í•  ì ê¸ˆ).")
    if scan_frequency: body_lines.append("- ìš”ì²­í•˜ì‹  **ë°˜ê¸° 1íšŒ ì´ìƒ** ê´€ë ¨ ì¡°í•­ì„ ì „ìˆ˜ ë§¤ì¹­í•´ ìš”ì§€ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    body_lines.append("- ìƒìœ„ë²• ìš°ì„  ì›ì¹™ì„ ì ìš©í–ˆê³ , ì§ì ‘ ê´€ë ¨ëœ í•˜ìœ„ë²•ë§Œ í¬í•¨í–ˆìŠµë‹ˆë‹¤.")
    body_lines.append("â€» ì¶”ê°€ í™•ì¸: ìƒÂ·í•˜ìœ„ë²•ì˜ **ìµœì‹ ê°œì •ì¼**ì´ ì„œë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë°˜ë“œì‹œ ë¹„êµí•´ ì£¼ì„¸ìš”.")
    body_md="\n".join(body_lines)

    return basis_md, body_md

def _compose_markdown(basis_md: str, body_md: str, disclaimer: str) -> str:
    return f"{basis_md}\n\n---\n{body_md}\n\n---\n> âš ï¸ **[ë©´ì±…ê³ ì§€]**\n> {disclaimer}"

@app.get("/answer", operation_id="answer")
def answer(
    keyword: str = Query(..., min_length=1),
    role_lock: Optional[str] = Query(None, description="ì—­í•  ì ê¸ˆ: ì•ˆì „ê´€ë¦¬ì/ë³´ê±´ê´€ë¦¬ì/ì•ˆì „ë³´ê±´ì´ê´„ì±…ì„ì/ê´€ë¦¬ê°ë…ì/ì•ˆì „ë³´ê±´ê´€ë¦¬ë‹´ë‹¹ì/ì‚°ì—…ë³´ê±´ì˜"),
    include_all_levels: bool = Query(True, description="ë²•Â·ë ¹Â·ê·œì¹™Â·ê³ ì‹œê¹Œì§€ ë‹¤ì¸µ ê·¼ê±° ì¶œë ¥"),
    scan_frequency: Optional[bool] = Query(None, description="â€˜ë°˜ê¸° 1íšŒ ì´ìƒâ€™ ë“± ë¹ˆë„ ì „ìˆ˜ ìŠ¤ìº”"),
):
    if scan_frequency is None:
        scan_frequency = bool(re.search(r"(ë°˜ê¸°|6\s*ê°œì›”|ë¶„ê¸°|1íšŒ\s*ì´ìƒ|ì •ê¸°)", keyword))

    basis_md, body_md = _compose_blocks(keyword, role_lock, include_all_levels, scan_frequency)
    markdown = _compose_markdown(basis_md, body_md, DISCLAIMER)

    return {
        "status":"ok","generated_at":_now_iso(),
        "legal_basis":basis_md,"middle":body_md,"disclaimer":DISCLAIMER,
        "markdown": markdown,
        "params":{"role_lock": role_lock or _detect_role(keyword),
                  "include_all_levels": include_all_levels,
                  "scan_frequency": scan_frequency}
    }

@app.get("/diag", operation_id="diag")
def diag():
    info={"sheets_id_set": bool(SHEETS_ID), "creds_path": GOOGLE_CREDS, "range": SHEETS_RANGE}
    try:
        creds=Credentials.from_service_account_file(
            GOOGLE_CREDS, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
        )
        svc=build("sheets","v4",credentials=creds)
        meta=svc.spreadsheets().get(spreadsheetId=SHEETS_ID).execute()
        titles=[s["properties"]["title"] for s in meta.get("sheets",[])]
        info.update({"ok": True, "sheet_titles": titles, "laws_loaded": len(LAWS)})
        return info
    except Exception as e:
        info.update({"ok": False, "error_type": e.__class__.__name__, "error": str(e)})
        return JSONResponse(info, status_code=500)
