# server.py  â€” v5.8 ì‘ë‹µ ê°•ì œ / Sheets ALL + í•œêµ­ì–´ í—¤ë” / ë‹¤ì¸µ ê·¼ê±° + ì—­í• ì ê¸ˆ + ì „ìˆ˜ìŠ¤ìº”
import os, glob, yaml, re, datetime, urllib.parse
from typing import List, Dict, Tuple, Optional
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build

# ---------- í™˜ê²½ë³€ìˆ˜ ----------
LAWS_DIR      = os.getenv("LAWS_DIR", "./laws")
SHEETS_ID     = os.getenv("SHEETS_SPREADSHEET_ID")
SHEETS_RANGE  = os.getenv("SHEETS_RANGE", "ALL")  # ALL ë˜ëŠ” 'íƒ­ëª…!A:Z, ë‹¤ë¥¸íƒ­!A:Z'
GOOGLE_CREDS  = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
TZ            = os.getenv("TZ", "Asia/Seoul")

DISCLAIMER = (
    "ë³¸ ë‹µë³€ì€ ì•ˆì „ë²•ë„ìš°ë¯¸ GPTê°€ ìƒì„±í•œ ì¼ë°˜ ì •ë³´ì´ë©°, ë²•ë¥  ìë¬¸ì´ë‚˜ ë²•ë¥ ì‚¬ë¬´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
    "íŠ¹ì • ì‚¬ì‹¤ê´€ê³„ì— ëŒ€í•œ í•´ì„Â·ì ìš©ì€ ê´€í• ê¸°ê´€ì˜ ê³µì‹ ì•ˆë‚´ì™€ ìê²© ìˆëŠ” ë³€í˜¸ì‚¬Â·ë…¸ë¬´ì‚¬ì˜ ìë¬¸ìœ¼ë¡œ ê²€ì¦í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. "
    "ë³¸ ëŒ€í™”ëŠ” ë³€í˜¸ì‚¬â€“ì˜ë¢°ì¸ ê´€ê³„ë¥¼ í˜•ì„±í•˜ì§€ ì•Šìœ¼ë©°, ì •ë³´ì˜ ìµœì‹ ì„±Â·ì™„ì „ì„±Â·ì í•©ì„±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
    "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê²°ì •ê³¼ ì‹¤í–‰ì˜ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤."
)

app = FastAPI(title="SafetyLawGPT API", version="1.1.0")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

LAWS: List[Dict] = []  # ë©”ëª¨ë¦¬ DB (ì‹œíŠ¸ ìš°ì„ , YAML ë³´ì¡°)

# ---------- ìœ í‹¸ ----------
def _strip_html(s: Optional[str]) -> str:
    return re.sub("<[^>]+>", "", s or "")

def _now_iso() -> str:
    return datetime.datetime.now().astimezone().isoformat()

def _safe(s: str) -> str:
    return (s or "").strip()

def _ellipsis(s: str, n: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= n else s[:n].rstrip() + "â€¦"

def _law_level(law_name: str) -> str:
    n = law_name or ""
    if "ì‹œí–‰ê·œì¹™" in n: return "rule"
    if "ì‹œí–‰ë ¹"  in n: return "decree"
    if "ê³ ì‹œ" in n or "ì§€ì¹¨" in n: return "notice"
    # 'ê¸°ì¤€ì— ê´€í•œ ê·œì¹™' ê°™ì€ í‘œí˜„ì€ ruleë¡œ ê°„ì£¼
    if "ê¸°ì¤€ì— ê´€í•œ ê·œì¹™" in n: return "rule"
    return "act"

def _safe_link(text: str, url: str) -> str:
    return f"[{text}](<{url}>)" if url else text

def _search_url(query: str) -> str:
    base = "https://www.law.go.kr/lsSc.do?section=&menuId=1&subMenuId=15&tabMenuId=81&eventGubun=060101&query="
    return base + urllib.parse.quote(query.strip())

ROLE_TOKENS = ["ì•ˆì „ê´€ë¦¬ì","ë³´ê±´ê´€ë¦¬ì","ì•ˆì „ë³´ê±´ì´ê´„ì±…ì„ì","ê´€ë¦¬ê°ë…ì","ì•ˆì „ë³´ê±´ê´€ë¦¬ë‹´ë‹¹ì","ì‚°ì—…ë³´ê±´ì˜"]

def _detect_role(q: str) -> Optional[str]:
    for r in ROLE_TOKENS:
        if r in q:
            return r
    return None

FREQ_PAT = re.compile(r"(ë°˜ê¸°\s*1íšŒ(?:\s*ì´ìƒ)?|ë°˜ê¸°|6ê°œì›”\s*1íšŒ(?:\s*ì´ìƒ)?)")
VERB_PAT = re.compile(r"(ì ê²€|í‰ê°€|ê´€ë¦¬|í™•ì¸|ê²€í† )")

def _scan_frequency(text: str) -> List[str]:
    out = []
    for m in FREQ_PAT.finditer(text or ""):
        # ì£¼ë³€ ë¬¸ë§¥ 60ì ì¶”ì¶œ
        start = max(0, m.start()-40); end = min(len(text), m.end()+40)
        ctx = text[start:end].replace("\n"," ").strip()
        if VERB_PAT.search(ctx):
            out.append(ctx)
    # ì¤‘ë³µ ì œê±°
    seen = set(); uniq = []
    for t in out:
        k = re.sub(r"\s+", " ", t)
        if k in seen: continue
        seen.add(k); uniq.append(t)
    return uniq[:20]

# ---------- YAML ë³´ì¡° ë¡œë” ----------
def _load_yaml(path: str) -> Optional[Dict]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    except Exception:
        return None

def load_from_yaml() -> List[Dict]:
    out = []
    for y in glob.glob(os.path.join(LAWS_DIR, "**", "*.yml"), recursive=True):
        r = _load_yaml(y)
        if not r or not r.get("law_id") or not r.get("article_no"):
            continue
        r["_text"]   = (r.get("text_plain") or _strip_html(r.get("text_html"))).strip()
        r["_source"] = "yaml"
        out.append(r)
    return out

# ---------- Sheets ë¡œë” (ALL íƒ­ + í•œêµ­ì–´ í—¤ë”) ----------
def _sheets_service():
    creds = Credentials.from_service_account_file(
        GOOGLE_CREDS, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
    )
    return build("sheets", "v4", credentials=creds)

def _ranges_from_env(svc):
    rng = (SHEETS_RANGE or "").strip()
    if rng.upper() == "ALL":
        meta = svc.spreadsheets().get(spreadsheetId=SHEETS_ID).execute()
        titles = [s["properties"]["title"] for s in meta.get("sheets", [])]
        return [f"{t}!A:Z" for t in titles]
    parts = [r.strip() for r in rng.split(",") if r.strip()]
    return parts if parts else ["Sheet1!A:Z"]

def _process_values(values: List[List[str]]) -> List[Dict]:
    if not values: 
        return []

    header = [h.strip() for h in values[0]]
    idx = {k:i for i,k in enumerate(header)}
    def ci(*names):
        for n in names:
            if n in idx: return idx[n]
        return None

    # í•œêµ­ì–´ í—¤ë” ëŒ€ì‘
    c_rev       = ci("ìµœì‹ ê°œì •ì¼","ê°œì •ì¼","revision_date")
    c_law_id    = ci("ë²•ë ¹ID","law_id","ID")
    c_law_name  = ci("ë²•ë ¹ëª…","law_name")
    c_article   = ci("ì¡°ë¬¸ë²ˆí˜¸","article_no")
    c_unit      = ci("ì¡°","unit","ì¡°ë¬¸êµ¬ë¶„","êµ¬ë¶„")
    c_title     = ci("ì¡°ë¬¸ì œëª©","article_title")
    c_text      = ci("ì¡°ë¬¸ë‚´ìš©(Plain)","text","ë³¸ë¬¸")
    c_html      = ci("ì¡°ë¬¸ë‚´ìš©(HTML)","text_html")
    c_deleted   = ci("ì‚­ì œì—¬ë¶€(Y/N)","ì‚­ì œì—¬ë¶€","ì‚­ì œ")
    c_src       = ci("ì¶œì²˜URL","source_url","URL","url")

    def g(row, col_index):
        return (row[col_index].strip() if col_index is not None and col_index < len(row) and row[col_index] is not None else "")

    by_key: Dict[Tuple[str,str], Dict] = {}

    for row in values[1:]:
        if _safe(g(row, c_deleted)).upper() == "Y":
            continue
        law_id     = _safe(g(row, c_law_id))
        law_name   = _safe(g(row, c_law_name))
        article_no = _safe(g(row, c_article))
        if not law_id or not article_no:
            continue

        unit_v   = _safe(g(row, c_unit))   # 'ì¡°' / 'í•­' / 'í˜¸' / 'ëª©'
        title    = _safe(g(row, c_title))
        revdate  = _safe(g(row, c_rev))
        src_url  = _safe(g(row, c_src))
        t_plain  = _safe(g(row, c_text))
        t_html   = _safe(g(row, c_html))

        key = (law_id, article_no)
        is_head = ("ì¡°" in unit_v) or (key not in by_key)

        if is_head:
            rec = {
                "law_id":        law_id,
                "law_name":      law_name,
                "article_no":    article_no,
                "article_title": title,
                "revision_date": revdate,
                "db_synced_at":  _now_iso().split("T")[0],
                "status":        "ìœ íš¨",
                "source_url":    src_url,
                "text_plain":    (t_plain + "\n") if t_plain else "",
                "text_html":     (t_html  + "\n") if t_html  else "",
                "_source":       "sheets",
            }
            by_key[key] = rec
        else:
            if t_plain: by_key[key]["text_plain"] += t_plain + "\n"
            if t_html:  by_key[key]["text_html"]  += t_html  + "\n"

    out = []
    for rec in by_key.values():
        rec["_text"] = (rec.get("text_plain") or _strip_html(rec.get("text_html"))).strip()
        rec["_level"] = _law_level(rec.get("law_name",""))
        out.append(rec)
    return out

def load_from_sheets() -> List[Dict]:
    if not (SHEETS_ID and GOOGLE_CREDS):
        return []
    try:
        svc = _sheets_service()
        ranges = _ranges_from_env(svc)
        resp = svc.spreadsheets().values().batchGet(spreadsheetId=SHEETS_ID, ranges=ranges).execute()
        valueRanges = resp.get("valueRanges", [])
        merged: List[Dict] = []
        for vr in valueRanges:
            merged.extend(_process_values(vr.get("values", [])))
        return merged
    except Exception:
        return []

# ---------- ì „ì²´ ë¦¬ë¡œë“œ ----------
def reload_all():
    global LAWS
    s = load_from_sheets()
    y = load_from_yaml()
    seen = set(); merged: List[Dict] = []
    for rec in s + y:   # ì‹œíŠ¸ê°€ ë¨¼ì €, ì¤‘ë³µí‚¤ëŠ” ì‹œíŠ¸ ìš°ì„ 
        key = (rec.get("law_id"), rec.get("article_no"))
        if key in seen: 
            continue
        seen.add(key)
        merged.append(rec)
    LAWS = merged

reload_all()

# ---------- ê°„ë‹¨ ê²€ìƒ‰ ----------
def _score(hay: str, kw: str) -> int:
    score = 0
    for t in kw.split():
        if t in hay: score += hay.count(t)
    if kw in hay: score += 3
    return score

def _search_local(keyword: str, limit: int = 12) -> List[Dict]:
    kw = keyword.strip()
    res: List[Tuple[int, Dict]] = []
    for r in LAWS:
        hay = f"{r.get('law_name','')} {r.get('article_no','')} {r.get('article_title','')} {r.get('_text','')}"
        sc = _score(hay, kw)
        if sc > 0:
            res.append((sc, r))
    res.sort(key=lambda x:x[0], reverse=True)
    return [x[1] for x in res[:limit]]

# ---------- ì—”ë“œí¬ì¸íŠ¸ ----------
@app.get("/healthz", operation_id="healthz")
def healthz():
    sheets = sum(1 for r in LAWS if r.get("_source")=="sheets")
    yaml_n = sum(1 for r in LAWS if r.get("_source")=="yaml")
    return {"ok": True, "ts": _now_iso(), "laws_loaded": len(LAWS), "sheets": sheets, "yaml": yaml_n}

@app.get("/reload", operation_id="reload")
def reload():
    reload_all()
    sheets = sum(1 for r in LAWS if r.get("_source")=="sheets")
    yaml_n = sum(1 for r in LAWS if r.get("_source")=="yaml")
    return {"ok": True, "reloaded": len(LAWS), "sheets": sheets, "yaml": yaml_n}

@app.get("/search", operation_id="search")
def search(keyword: str = Query(..., min_length=1), limit: int = 10):
    hits = _search_local(keyword, limit)
    return {"count": len(hits), "items": [
        {"law_id": h.get("law_id"), "law_name": h.get("law_name"),
         "article_no": h.get("article_no"), "title": h.get("article_title"),
         "revision_date": h.get("revision_date"), "db_synced_at": h.get("db_synced_at"),
         "level": h.get("_level"), "source": h.get("_source")}
        for h in hits]}

def _group_by_level(hits: List[Dict]) -> Dict[str, List[Dict]]:
    buckets: Dict[str, List[Dict]] = {"act":[], "decree":[], "rule":[], "notice":[]}
    for r in hits:
        buckets.setdefault(r.get("_level","act"), []).append(r)
    # ê° ë ˆë²¨ ìƒìœ„ 3ê°œê¹Œì§€ë§Œ
    for k in buckets:
        buckets[k] = buckets[k][:3]
    return buckets

def _summarize(text: str) -> str:
    t = re.sub(r"\s+", " ", (text or "").strip())
    return _ellipsis(t, 220)

def _basis_block_for(rec: Dict, scan_freq: bool=False) -> str:
    # ë°˜ë§ì²´ë¡œ ì‘ì„±
    law_name = rec.get("law_name","")
    article  = rec.get("article_no","")
    title    = rec.get("article_title","")
    rev      = rec.get("revision_date","")
    src      = rec.get("source_url","")
    summary  = _summarize(rec.get("_text",""))
    lines = []
    lines.append(f"- **ë²•ë ¹ëª…:** {law_name}")
    lines.append(f"- **ì¡°ë¬¸:** {article}({title})")
    lines.append(f"- **ìµœì‹ ê°œì •ì¼:** {rev}")
    lines.append(f"- **ì›ë¬¸ ìš”ì§€:** {summary}")
    if scan_freq:
        matches = _scan_frequency(rec.get("_text",""))
        if matches:
            lines.append(f"- **ã€ˆë§¤ì¹­ í•­ëª©(ì „ìˆ˜)ã€‰**")
            for m in matches:
                mm = _ellipsis(m, 140)
                lines.append(f"  - â€œ**{mm}**â€")
    lines.append(f"- **ì¶œì²˜:** {_safe_link('êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° ë°”ë¡œê°€ê¸°', src)}")
    return "\n".join(lines)

def _compose_blocks(keyword: str, role_lock: Optional[str], include_all_levels: bool, scan_frequency: bool) -> Tuple[str,str]:
    hits = _search_local(keyword, 16)
    if not hits:
        srch = _search_url(keyword)
        basis = f"**[ê·¼ê±°]**\n- ì›ë¬¸ì„ ì°¾ì§€ ëª»í–ˆë‹¤. ë‚´ë¶€ DB(ì‹œíŠ¸/ë¡œì»¬)ì— í•´ë‹¹ ì¡°ë¬¸ì´ ì—†ë‹¤.\n- **ê²€ìƒ‰ ê²½ë¡œ:** {_safe_link('êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° ê²€ìƒ‰', srch)}"
        body  = ("**ì§ˆë¬¸ í•´ê²° ìš”ì•½**\n"
                 "- ë²•ë¥  â†’ ì‹œí–‰ë ¹ â†’ ì‹œí–‰ê·œì¹™ â†’ ê³ ì‹œÂ·ì§€ì¹¨ ìˆœì„œë¡œ ìµœì‹  ì›ë¬¸ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.\n"
                 "- ì¡°ë¬¸Â·ë³„í‘œ ì •í™• ì¼ì¹˜ í•­ëª©ë§Œ ì¸ìš©í•©ë‹ˆë‹¤.\n"
                 "â€» ì¶”ê°€ í™•ì¸: ìƒÂ·í•˜ìœ„ë²• ê°œì •ì¼ì„ ê¼­ ë¹„êµí•´ ì£¼ì„¸ìš”.")
        return basis, body

    # ì—­í•  ì ê¸ˆ: ì§ˆì˜ì— ì—­í•  í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´, ê·¸ ì—­í•  ê´€ë ¨ ê¸€ì í¬í•¨ ë ˆì½”ë“œ ìš°ì„ 
    role = role_lock or _detect_role(keyword) or ""
    if role:
        role_hits = [r for r in hits if role in (r.get("_text","")+r.get("article_title","")+r.get("law_name",""))]
        if role_hits:
            hits = role_hits + [r for r in hits if r not in role_hits]

    buckets = _group_by_level(hits)
    order = ["act","decree","rule","notice"] if include_all_levels else ["decree"]  # ìµœì†Œ ì‹œí–‰ë ¹
    labels = {"act":"(ë²•ë¥ )","decree":"(ì‹œí–‰ë ¹)","rule":"(ì‹œí–‰ê·œì¹™)","notice":"(ê³ ì‹œÂ·ì§€ì¹¨)"}

    basis_parts = ["ğŸ“Œ **[ê·¼ê±°]**"]
    for lv in order:
        if not buckets.get(lv): continue
        for rec in buckets[lv]:
            basis_parts.append(f"- **{labels[lv]}**")
            basis_parts.append(_basis_block_for(rec, scan_freq=scan_frequency))
    basis_md = "\n".join(basis_parts)

    # ë³¸ë¬¸(ì¡´ëŒ“ë§)
    body_lines = []
    body_lines.append("**ë‚´ìš© ìš”ì•½**")
    if role:
        body_lines.append(f"- ë³¸ ì§ˆì˜ëŠ” **{role}** ê´€ë ¨ìœ¼ë¡œ í•´ì„í–ˆìŠµë‹ˆë‹¤(ì—­í•  ì ê¸ˆ).")
    if scan_frequency:
        body_lines.append("- ìš”ì²­í•˜ì‹  **ë°˜ê¸° 1íšŒ ì´ìƒ** ê´€ë ¨ ì¡°í•­ì„ ì „ìˆ˜ë¡œ ë§¤ì¹­í•˜ì—¬ ìš”ì§€ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    body_lines.append("- ìƒìœ„ë²• ìš°ì„  ì›ì¹™ì„ ì ìš©í–ˆìœ¼ë©°, ì§ì ‘ ê´€ë ¨ëœ í•˜ìœ„ë²•ë§Œ í¬í•¨í–ˆìŠµë‹ˆë‹¤.")
    body_lines.append("â€» ì¶”ê°€ í™•ì¸: ìƒÂ·í•˜ìœ„ë²•ì˜ **ìµœì‹ ê°œì •ì¼**ì´ ì„œë¡œ ë‹¤ë¥¸ ê²½ìš°ê°€ ìˆìœ¼ë‹ˆ ë°˜ë“œì‹œ ê°œì •ì¼ì„ ë¹„êµí•´ ì£¼ì„¸ìš”.")
    body_md = "\n".join(body_lines)

    return basis_md, body_md

def _compose_markdown(basis_md: str, body_md: str, disclaimer: str) -> str:
    # v5.8 3ë¸”ë¡ ê°•ì œ
    return f"{basis_md}\n\n---\n{body_md}\n\n---\n> âš ï¸ **[ë©´ì±…ê³ ì§€]**\n> {disclaimer}"

@app.get("/answer", operation_id="answer")
def answer(
    keyword: str = Query(..., min_length=1),
    role_lock: Optional[str] = Query(None, description="ì—­í•  ì ê¸ˆ: ì•ˆì „ê´€ë¦¬ì/ë³´ê±´ê´€ë¦¬ì/ì•ˆì „ë³´ê±´ì´ê´„ì±…ì„ì/ê´€ë¦¬ê°ë…ì/ì•ˆì „ë³´ê±´ê´€ë¦¬ë‹´ë‹¹ì/ì‚°ì—…ë³´ê±´ì˜"),
    include_all_levels: bool = Query(True, description="ë²•Â·ë ¹Â·ê·œì¹™Â·ê³ ì‹œê¹Œì§€ ë‹¤ì¸µ ê·¼ê±° ì¶œë ¥"),
    scan_frequency: Optional[bool] = Query(None, description="â€˜ë°˜ê¸° 1íšŒ ì´ìƒâ€™ ë“± ë¹ˆë„ ì „ìˆ˜ ìŠ¤ìº”"),
):
    # scan_frequency ìë™ íŒë³„
    if scan_frequency is None:
        scan_frequency = bool(re.search(r"(ë°˜ê¸°|6ê°œì›”|1íšŒ\s*ì´ìƒ)", keyword))

    basis_md, body_md = _compose_blocks(keyword, role_lock, include_all_levels, scan_frequency)
    markdown = _compose_markdown(basis_md, body_md, DISCLAIMER)

    return {
        "status": "ok",
        "generated_at": _now_iso(),
        # v5.8 3ë¸”ë¡ í˜¸í™˜ í•„ë“œ
        "legal_basis": basis_md,     # [ê·¼ê±°]
        "middle": body_md,           # (ì œëª© ì—†ëŠ” ë³¸ë¬¸)
        "disclaimer": DISCLAIMER,    # [ë©´ì±…ê³ ì§€]
        # í†µí•© ë§ˆí¬ë‹¤ìš´(ì§ì ‘ ë Œë”ë§ìš©)
        "markdown": markdown,
        # ë””ë²„ê·¸ íŒíŠ¸
        "params": {
            "role_lock": role_lock or _detect_role(keyword),
            "include_all_levels": include_all_levels,
            "scan_frequency": scan_frequency
        }
    }

@app.get("/diag", operation_id="diag")
def diag():
    info = {"sheets_id_set": bool(SHEETS_ID), "creds_path": GOOGLE_CREDS, "range": SHEETS_RANGE}
    try:
        creds = Credentials.from_service_account_file(
            GOOGLE_CREDS, scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
        )
        svc = build("sheets", "v4", credentials=creds)
        meta = svc.spreadsheets().get(spreadsheetId=SHEETS_ID).execute()
        titles = [s["properties"]["title"] for s in meta.get("sheets", [])]
        info.update({"ok": True, "sheet_titles": titles, "laws_loaded": len(LAWS)})
        return info
    except Exception as e:
        info.update({"ok": False, "error_type": e.__class__.__name__, "error": str(e)})
        return JSONResponse(info, status_code=500)

